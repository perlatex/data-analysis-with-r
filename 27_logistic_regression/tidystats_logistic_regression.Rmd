---
title: "logistic regression"
author: "王小二"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: flatly
    highlight: pygments
    code_download: true
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
---


前一节课讲了线性回归，课后有同学问我，怎么理解“回归”的意思。

> 我们生活总也经常用这个词，比如，回归生活、回归家庭，回归到原来的状态，回到出发时候的状态等等。我们数据分析中的回归，差不多也是这个意思。

> 可以假设，图中的每一个点原初都是从一条**看不见的直线**对应位置上跳出去的，回归的意思就是，让每个点都回到这条直线上去，找到这条看不见的直线，就是我们的任务。



线性模型需要满足四个假设条件。如果某个条件不满足呢，比如，我们的被解释变量是**考试通过**和**不通过**二元变量，显然，不满足线性关系，这种情形应该怎么处理呢？先看一个案例。



# 案例

假定我们收集了一些研究生入学申请的数据

```{r, message=FALSE, warning=FALSE}
library(tidyverse)

gredata <- read_csv("data/gredata.csv") %>%
  mutate(rank = as.factor(rank))

gredata
```



数据变量的解释：

- `admit` : 是否被录取(admit = 1被录取；admit = 0没有被录取)
- `gre`   : 学生的研究生考试成绩(Graduate Record Exam scores)
- `gpa`   : 平均成绩点数(grade point average)
- `rank`  : 学生本科院校的声望（1代表最高，有点类似国内的一本二本）



# 问题

`录取是否`与学生GRE考试成绩有关系？我们能用GRE成绩**预测**录取情况吗？


## 探索

我们注意到，这里的被解释变量(录取与否)只有两个值：0 和 1

$$
y_i = 
\begin{cases}
 1 & \text{if student i was admitted} \\
 0 & \text{if student i was not admitted} 
\end{cases}
$$


```{r}
gredata %>%
  mutate(admit = as.factor(admit)) %>%
  ggplot(aes(admit, gre, fill = admit)) +
  geom_boxplot() +
  theme_bw()
```


## 线性模型失效

```{r, echo = FALSE, fig.cap = "Linear vs. logistic regression models for binary response data"}
knitr::include_graphics("images/OLSlogistic-1.png")
```


```{r}
gredata %>%
  ggplot(aes(gre, admit)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()
```


**解释变量**和**被解释变量**不是线性关系，因此不能用线性回归。



# 先和小姐姐玩个游戏

## 游戏规则

薇薇小姐姐玩游戏，薇薇手上有6个硬币，她每次会把手上的6个硬币硬币一次性抛出，然后数正面朝上的硬币个数。这个游戏可以用如下代码模拟

```{r}
rbinom(n = 3, size = 6, prob = 0.5)
```

- `size = 6`，  小姐姐手上有6个硬币
- `prob = 0.5`，每个硬币正面出现的概率为0.5，(有时候正面记为1，反面记为0)
- `n    = 3`，  玩3次

玩一次就返回一次硬币正面朝上的个数，玩了3次就返回3个结果。



## 是否被录取，可以看做是抛硬币

薇薇小姐姐把硬币奖励给了听话的小朋友，手上只剩下1个硬币。现在，她有了新的使命，她要抛这个硬币来决定某位同学是否被录取，当硬币出现正面就录取，出现反面就不录取。假定有三个同学报考研究生，所以薇薇姐要抛抛`n = 3`次。

```{r}
rbinom(n = 3, size = 1, prob = 0.5)
```

这三个同学的录取概率都是0.5，有同学不服气，如果这样，努力还有什么用呢，每个人的录取概率应该是不一样的，所以我们这里概率修改下为 `c(0.2, 0.5, 0.85)`

```{r}
rbinom(3, size = 1, prob = c(0.2, 0.5, 0.85))
```

`是否录取`可以看做是一个概率过程。



## 神奇的硬币

现实中的硬币，正反面出现的概率是0.5。但是，我们这里的硬币是一个神奇的硬币，它能感知我们的努力，成绩越高它朝上的概率越大，成绩越低它朝上的概率就越低。那么，问题来了，GRE成绩或者成绩的线性组合，怎么转换成概率呢?


假定三位同学的R语言成绩是（请放心，大家的成绩不会是30分）

```{r}
scores <- c(30, 60, 90)
```

成绩的线性组合是（这里的截距`-4.5`和系数`0.08`，也是我瞎掰的）
```{r}
linear <- -4.5 + 0.08 * scores
linear
```


可以利用下面这个伟大的**函数**，将线性组合值转换成**概率**
```{r}
inv_logit <- function(x) {
  exp(x) / (1 + exp(x))
}

inv_logit(linear)
```

可以看到，成绩越高，录取的概率越大。


搞定。现在终于打通了logistic回归的思路：

`成绩 --> 成绩的线性组合 --> 概率 --> 是否录取(0/1)`

 <here>                     <here>

| GRE 	| linear 	| Prob 	| admit |
|-----	|--------	|------	|:-----:|
| 850 	|        	|      	|   0   |
| 657 	|        	|      	|   1   |
| 891 	|        	|      	|   1   |



# logistic回归

## logistic回归的数学表达式

$$
\begin{align*}
\text{admit}_{i} &\sim \mathrm{Binomial}(1, \;p_{i}) \\
\text{logit}(p_{i}) &= \alpha + \beta \cdot \text{gre}_{i} \\
\text{equivalent to,} \quad p_{i} &= \text{logit}^{-1}(\alpha + \beta \cdot\text{gre}_{i}) \\
\end{align*}
$$



- 这里 $p_i$ 就是被录取的概率。


- `logit()`是连接函数（还记得前面那个伟大的函数？伟大的函数就是这个连接函数的反函数），是线性组合值与概率值的桥梁。它的定义如下
$$
\begin{align*}
\text{logit}(p_{i}) &= \log\Big(\frac{p_{i}}{1 - p_{i}}\Big)
\end{align*}
$$

```{r}
logit <- function(x) log(x / (1 - x))
```

- 预测变量 $gre$ 的线性组合**模拟的**正是连接函数的返回值

$$
\begin{align*}
\text{logit}(p_{i}) &= \log\Big(\frac{p_{i}}{1 - p_{i}}\Big)= \alpha + \beta \cdot \text{gre}_{i} \\
\end{align*}
$$

有时也称$\text{logit}(p_{i})$为概率的对数比率(log-odds).



## 代码实现

按照上面表达式，用**glm**函数写代码，(generalized linear models)

```{r}
model_logit <- glm(
  admit ~ 1 + gre,
  data = gredata,
  family = binomial(link = "logit")
)

summary(model_logit)
```



得到`gre`的系数是

```{r}
coef(model_logit)
```

怎么理解这个0.003582呢？



### 系数解释

先上神器

```{r}
library(equatiomatic)
equatiomatic::extract_eq(model_logit)
```


上面的等式可以写成

$$
\begin{align*}
\operatorname{logit}\big[P( \operatorname{admit} = 1) \big] = \alpha + \beta_{1}(\operatorname{gre}) \\
\end{align*}
$$


> 这个系数对应着，预测因子gre增加一个单位时，录取概率的对数比率(log-odds)的变化。


有点难以理解，好吧。

我们把`logit`移到等式右边，等式左边由**概率的对数比率**变成**概率**，更符合人的直觉。上面公式可以写成

$$
 P( \operatorname{admit} = \operatorname{1} )  = \text{logit}^{-1}(\alpha + \beta_{1}\operatorname{gre})
$$

> gre成绩的线性组合，经过`inv_logit`（连接函数的反函数）计算后，等于该成绩对应的录取概率。

具体来说：

-  GRE分数200分左右，录取概率约0.1；
```{r}
inv_logit(coef(model_logit)[1] + coef(model_logit)[2] * 200)
```

-  GRE分数500分左右，录取概率约0.25；
```{r}
inv_logit(coef(model_logit)[1] + coef(model_logit)[2] * 500)
```

-  GRE分数800分左右，录取概率接近0.5；
```{r}
inv_logit(coef(model_logit)[1] + coef(model_logit)[2] * 800)
```


-  GRE分数从500分提高到800分，录取概率增加了接近0.25
```{r}
inv_logit(coef(model_logit)[1] + coef(model_logit)[2] * 800)  - inv_logit(coef(model_logit)[1] + coef(model_logit)[2] * 500)
```


**课堂练习**，计算GRE为900分的录取概率



### 拟合

拟合，就是计算原数据中每个`gre`值对应的录取概率。下面，我们用三种方式计算

1. 手工完成

```{r}
tbl <- gredata %>%
  mutate(
    prob = inv_logit(coef(model_logit)[1] + coef(model_logit)[2] * gre)
  )
tbl
```

```{r}
tbl %>%
  ggplot(aes(x = gre, y = prob)) +
  geom_line(color = "red", size = 2) +
  theme_minimal()
```

可以看到，GRE分数对录取的概率的影响是**正的且非线性的**。




2. 用`fitted()`或`predict()`
```{r}
gredata %>%
  mutate(
    fitted = fitted(model_logit)
  )
```

或者
```{r}
gredata %>%
  mutate(
    pred = predict(model_logit, type = "response")
  )
```



3. 使用`broom`包
```{r}
logit_probs <- model_logit %>% 
  broom::augment(
    data = gredata,
    type.predict = c("response")
) 

logit_probs
```



```{r}
logit_probs %>%
  ggplot(aes(x = gre, y = .fitted)) +
  geom_line(color = "#CF4446", size = 2) +
  theme_minimal()
```



```{r}
library(ggdist)

gredata %>%
  ggplot(aes(x = gre, y = admit, side = ifelse(admit == 1, "bottom", "top"))) +
  geom_dots(scale = 0.5) +
  geom_line(
    data = logit_probs, 
    aes(x = gre, y = .fitted),
    color = "#CF4446", size = 2
  ) +
  labs(
    title = "logit dotplot: stat_dots() with stat_lineribbon()",
    subtitle = 'aes(side = ifelse(admit == 1, "bottom", "top"))',
    x = "GRE score",
    y = "Pr(admit = 1)"
  )
```

### 预测

预测，就是指定新的`gre`值，求出对应的录取概率。这就需要先准备一个数据框，包含从低到高变化的`gre`成绩，然后利用模型计算相应的概率值。这里也有几个方法


1. 手动完成

```{r}
tibble(
  gre = c(200, 500, 700, 800)
) %>% 
  mutate(
    prob = inv_logit(coef(model_logit)[1] + coef(model_logit)[2] * gre)
  )
  
```


2. 用`predict()`
```{r}
newdata <- tibble(
  gre = c(200, 500, 700, 800)
)

newdata %>%
  mutate(
    pred_prob = predict(model_logit, newdata = newdata, type = "response"),
  )
```


3. 用`broom`包，更顺滑（注意，预测时用的参数是`newdata =` ）
```{r}
model_logit %>%
  broom::augment(
    newdata = newdata,       #<< newdata = , not data =
    type.predict = c("response"),
    se_fit = TRUE
  )
```




## 更多模型

我们增加更多的解释变量

```{r}
model_logit2 <- glm(
  admit ~ 1 + gre + gpa + rank,
  data = gredata,
  family = binomial(link = "logit")
)

summary(model_logit2)
```


**课堂练习**，调整`rank`因子顺序，让`rank4`作为基线




### 简单预测

模型 `model_logit2` 是`gre`, `gpa`和`rank`三个解释变量， 因此，在模型预测的时候，提供的新数据框也应该包含 `gre`, `gpa`和`rank`三列。简单起见，

- `gre`值为所有同学的平均`gre`值；
- `gpa`值为所有同学的平均`gpa`值；
- `rank`为学校声望的四个等级。

```{r}
newdata2 <-
  data.frame(
    gre = mean(gredata$gre),
    gpa = mean(gredata$gpa),
    rank = factor(1:4)
  )
newdata2
```



```{r}
model_logit2 %>%
  broom::augment(
    newdata = newdata2,
    type.predict = c("response"), # probability scale
    se_fit = TRUE
  )
```

说明处于平均`gre`值和平均`gpa`值水平的学生，本科院校越好，获录取的概率越大。



**课堂练习**， 假定来自一本院校、`gre`成绩600分，`gpa`成绩分布是`c(2, 3, 4, 5)`的四位同学，录取的概率分别是多大？


```{r}
nd <- tibble(
  gre = 600,
  gpa = c(2, 3, 4, 5),
  rank = factor(1, levels = c(1, 2, 3, 4))
)

model_logit2 %>%
  broom::augment(
    newdata = nd,
    type.predict = c("response"), 
    se_fit = TRUE
  )
```



### 复杂预测

在每个`rank`学校声望等级下，让 `gpa` 保持均值水平，`gre`成绩从200到800变化，构造成新的数据框

```{r}
newdata3 <-
  data.frame(
    gre = seq(from = 200, to = 800, length.out = 100),
    gpa = mean(gredata$gpa)
  ) %>%
  crossing(
    rank = factor(1:4)
  )

newdata3
```

由此可以画出，来自不同声望等级院校的学生，研究生录取概率随GRE成绩的变化。这里为了画出置信区间，我们并不是让模型直接给出录取概率，而是让模型先给出预测因子线性组合的值及其标准误，计算置信区间后，最后利用连接函数的反函数(`inv_logit()`) 转换到概率值，具体代码如下

```{r}
model_logit2 %>%
  broom::augment(
    newdata = newdata3,
    type.predict = c("link"),         ## linear scale
    se_fit = TRUE
  ) %>%
  mutate(
     prob = inv_logit(.fitted),
    lower = inv_logit(.fitted - (1.96 * .se.fit)),
    upper = inv_logit(.fitted + (1.96 * .se.fit))
  ) %>%
  ggplot(aes(x = gre, y = prob)) +
  geom_ribbon(
    aes(ymin = lower, ymax = upper, fill = rank),
    alpha = 0.3
  ) +
  geom_line(aes(colour = rank), size = 1) +
  theme_minimal()
```



# 参考

- <https://broom.tidymodels.org/reference/augment.glm.html>



# 用贝叶斯重做model_logit1

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidybayes)
library(rstan)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
theme_set(bayesplot::theme_default())
```

```{r}
stan_program <- "
data {
  int<lower=0> N;
  vector[N] x;
  int<lower=0,upper=1> y[N];
  int<lower=0> M;
  vector[M] new_x;  
}
parameters {
  real alpha;
  real beta;
}
model {
  // more efficient and arithmetically stable
  y ~ bernoulli_logit(alpha + beta * x);
}
generated quantities {
  vector[M] y_epred; 
  vector[M] mu = alpha + beta * new_x;

  for(i in 1:M) {
    y_epred[i] = inv_logit(mu[i]);
  }
   
}
"

newdata <- data.frame(
    gre = seq(min(gredata$gre), max(gredata$gre), length.out = 100)
   ) 


stan_data <- list(
  N = nrow(gredata),
  y = gredata$admit, 
  x = gredata$gre,
  M = nrow(newdata),
  new_x = newdata$gre
)

m <- stan(model_code = stan_program, data = stan_data)
```

```{r}
fit <- m %>%
  tidybayes::gather_draws(y_epred[i]) %>%
  ggdist::mean_qi(.value)
fit
```

```{r}
fit %>% 
  bind_cols(newdata) %>% 
  ggplot(aes(x = gre)) +
  geom_dots(
    data = gredata,
    aes(y = admit, side = ifelse(admit == 1, "bottom", "top")),
    scale = 0.4
  ) +
  geom_lineribbon(
    aes(y = .value, ymin = .lower, ymax = .upper), 
    alpha = 1/4, 
    fill = "#08306b"
  ) +
  labs(
    title = "logit dotplot: stat_dots() with stat_lineribbon()",
    subtitle = 'aes(side = ifelse(admit == 1, "bottom", "top"))',
    x = "GRE score",
    y = "Pr(admit = 1)"
  )
```

我还是愿意拥抱贝叶斯。